{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65deaa4",
   "metadata": {},
   "source": [
    "### DNABERT ì½”ë“œ (ì ìˆ˜ : 0.531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49228655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import amp\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 0) ì„¤ì • (ê²½ë¡œ & í•˜ì´í¼íŒŒë¼ë¯¸í„°)\n",
    "# =========================================\n",
    "SEED = 42\n",
    "\n",
    "# âœ… DNABERT-6 (k=6) ë°±ë³¸\n",
    "MODEL_ID = \"zhihan1996/DNA_bert_6\"\n",
    "\n",
    "# ğŸ”§ ë„¤ PC ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "DATA_DIR = r\"C:\\Users\\yutt4\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¸ê³µì§€ëŠ¥\\open\"\n",
    "GRCH38_CSV = os.path.join(DATA_DIR, \"grch38_windows_seq.csv\")\n",
    "\n",
    "TEST_PATH       = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SAMPLE_SUB_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "OUT_PATH        = os.path.join(DATA_DIR, \"submission_dnabert6_multipos_infonce_grch38_ëª¨ë¸ ë³€ê²½.csv\")\n",
    "\n",
    "# í•™ìŠµìš© í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "MAX_GRCH_SEQ   = 300_000   # GRCh38ì—ì„œ ìµœëŒ€ ì‚¬ìš©í•  ì‹œí€€ìŠ¤ ê°œìˆ˜\n",
    "CHUNK_SIZE     = 200_000   # ìŠ¤íŠ¸ë¦¬ë° ë¡œë”© chunksize\n",
    "MAX_LENGTH     = 512       # DNABERT í† í°(max_length)\n",
    "KMER           = 6         # DNABERT-6 â†’ k=6\n",
    "LAST_N_LAYERS  = 4         # ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´ ê°€ì¤‘í•©\n",
    "OUTPUT_DIM     = 512       # ì œì¶œ ì„ë² ë”© ì°¨ì› (<= 2048 ê·œì¹™)\n",
    "\n",
    "TRAIN_EPOCHS   = 4         # í•™ìŠµ epoch ìˆ˜ (ì‹œê°„ ë³´ê³  ì¡°ì ˆ)\n",
    "NUM_TUPLES     = 60_000    # epochë‹¹ (anchor, pos1, pos2) íŠœí”Œ ê°œìˆ˜\n",
    "BATCH_SIZE_TR  = 32        # í•™ìŠµ ë°°ì¹˜\n",
    "BATCH_SIZE_INFER = 32      # ì¶”ë¡  ë°°ì¹˜\n",
    "\n",
    "LR_HEAD        = 3e-4\n",
    "WEIGHT_DECAY   = 1e-4\n",
    "TEMPERATURE    = 0.07\n",
    "USE_FP16       = True\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 1) ìœ í‹¸ í•¨ìˆ˜ë“¤\n",
    "# =========================================\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    return x / x.norm(p=2, dim=-1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "_rc_map = str.maketrans(\"ACGT\", \"TGCA\")\n",
    "def reverse_complement(seq: str) -> str:\n",
    "    return seq.translate(_rc_map)[::-1]\n",
    "\n",
    "\n",
    "def clean_seq(seq: str) -> str:\n",
    "    # N ì œê±° + ëŒ€ë¬¸ì\n",
    "    s = str(seq).upper().replace(\"N\", \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def crop_seq(seq: str, max_len_nt: int) -> str:\n",
    "    \"\"\"\n",
    "    ë‰´í´ë ˆì˜¤íƒ€ì´ë“œ ê¸°ì¤€ìœ¼ë¡œ crop. (k-mer ì „ì— ìë¥´ê¸°)\n",
    "    max_len_ntëŠ” ëŒ€ëµ k * MAX_LENGTH ë³´ë‹¤ ì¡°ê¸ˆ í¬ê²Œ ì¡ì„ ìˆ˜ë„ ìˆì§€ë§Œ\n",
    "    ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ 512~1000 ì •ë„ë¡œ ì‚¬ìš©í•´ë„ ë¬´ë°©.\n",
    "    \"\"\"\n",
    "    if len(seq) <= max_len_nt:\n",
    "        return seq\n",
    "    start = random.randint(0, len(seq) - max_len_nt)\n",
    "    return seq[start:start + max_len_nt]\n",
    "\n",
    "\n",
    "def apply_snv(seq: str, k: int) -> str:\n",
    "    \"\"\"\n",
    "    seqì—ì„œ kê°œì˜ ìœ„ì¹˜ë¥¼ ë‹¤ë¥¸ ì—¼ê¸°ë¡œ ì¹˜í™˜ (SNV augmentation)\n",
    "    \"\"\"\n",
    "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    if len(seq) == 0:\n",
    "        return seq\n",
    "    s = list(seq)\n",
    "    idxs = random.sample(range(len(s)), min(k, len(s)))\n",
    "    for idx in idxs:\n",
    "        orig = s[idx]\n",
    "        cand = [b for b in bases if b != orig]\n",
    "        if cand:\n",
    "            s[idx] = random.choice(cand)\n",
    "    return \"\".join(s)\n",
    "\n",
    "\n",
    "def to_kmer(seq: str, k: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    DNABERT-6ëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ k-mer ì‹œí€€ìŠ¤ë¥¼ í† í°í™”í•¨.\n",
    "    ì˜ˆ: \"ACGTG...\" â†’ \"ACGTGA CGTGAC ...\"\n",
    "    \"\"\"\n",
    "    seq = clean_seq(seq)\n",
    "    if len(seq) < k:\n",
    "        return seq\n",
    "    kmers = [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "    return \" \".join(kmers)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2) GRCh38 ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ\n",
    "# =========================================\n",
    "def load_grch38_sequences_stream(path: str,\n",
    "                                 max_samples: int,\n",
    "                                 chunksize: int = 200_000) -> List[str]:\n",
    "    \"\"\"\n",
    "    GRCh38 windowsë¥¼ chunk ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ,\n",
    "    ìµœëŒ€ max_samples ê°œì˜ cleanëœ seqë§Œ ìˆ˜ì§‘.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    total_rows = 0\n",
    "\n",
    "    print(f\"\\n[GRCh38 STREAM LOAD] {path}\")\n",
    "    for chunk in pd.read_csv(path, usecols=[\"seq\"], chunksize=chunksize):\n",
    "        total_rows += len(chunk)\n",
    "        seqs = [clean_seq(s) for s in chunk[\"seq\"].astype(str).tolist()]\n",
    "        seqs = [s for s in seqs if len(s) > 0]\n",
    "\n",
    "        needed = max_samples - len(sequences)\n",
    "        if needed <= 0:\n",
    "            break\n",
    "\n",
    "        if len(seqs) > needed:\n",
    "            seqs = seqs[:needed]\n",
    "\n",
    "        sequences.extend(seqs)\n",
    "\n",
    "        del chunk\n",
    "        if len(sequences) >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(f\"Total rows read (approx): {total_rows}\")\n",
    "    print(\"GRCh38 sequences used:\", len(sequences))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3) multi-positive íŠœí”Œ ìƒì„±\n",
    "# =========================================\n",
    "def generate_multi_pos_tuples(seqs: List[str], num_tuples: int) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    ê° íŠœí”Œ: (anchor, pos1, pos2)\n",
    "      - anchor: GRCh38ì—ì„œ ëœë¤ seq í•˜ë‚˜ ì‚¬ìš© (crop ì¶”ê°€)\n",
    "      - pos1  : anchorì— 1~2ê°œì˜ SNV\n",
    "      - pos2  : reverse complement(anchor SNV ë²„ì „)\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_tuples} multi-positive tuples (anchor, pos1, pos2)\")\n",
    "    tuples = []\n",
    "    max_anchor_nt = 512  # anchor ë‰´í´ë ˆì˜¤íƒ€ì´ë“œ ê¸¸ì´ ìƒí•œ (ì ë‹¹íˆ)\n",
    "\n",
    "    while len(tuples) < num_tuples:\n",
    "        anchor_raw = random.choice(seqs)\n",
    "        anchor = crop_seq(clean_seq(anchor_raw), max_anchor_nt)\n",
    "        if len(anchor) < KMER:\n",
    "            continue\n",
    "\n",
    "        # pos1: ì‘ì€ SNV (1~2ê°œ)\n",
    "        pos1 = apply_snv(anchor, random.randint(1, 2))\n",
    "\n",
    "        # pos2: RC + SNV (1ê°œ ì •ë„)\n",
    "        anchor_snv_for_rc = apply_snv(anchor, 1)\n",
    "        pos2 = reverse_complement(anchor_snv_for_rc)\n",
    "\n",
    "        tuples.append((anchor, pos1, pos2))\n",
    "\n",
    "    return tuples\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 4) DNABERT ì„ë² ë”© í—¤ë“œ\n",
    "# =========================================\n",
    "class DnaBertMultiLayerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    - DNABERT hidden_statesì˜ ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë¥¼ learnable weightë¡œ ê°€ì¤‘í•©\n",
    "    - attention_mask ê¸°ë°˜ mean pooling\n",
    "    - OUTPUT_DIMìœ¼ë¡œ íˆ¬ì˜ í›„ L2 normalize\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, last_n: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.last_n = last_n\n",
    "        self.layer_weights = nn.Parameter(torch.zeros(last_n))\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask: torch.Tensor):\n",
    "        # hidden_states: list of (B, L, H), len = num_layers+1\n",
    "        hs = hidden_states[-self.last_n:]\n",
    "        stack = torch.stack(hs, dim=0)  # (N, B, L, H)\n",
    "\n",
    "        w = F.softmax(self.layer_weights, dim=0).view(-1, 1, 1, 1)\n",
    "        feat = (stack * w).sum(dim=0)   # (B, L, H)\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).float()  # (B, L, 1)\n",
    "        feat = feat * mask\n",
    "\n",
    "        summed = feat.sum(dim=1)                     # (B, H)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)      # (B, 1)\n",
    "        mean_emb = summed / denom                    # (B, H)\n",
    "\n",
    "        return l2_normalize(self.proj(mean_emb))     # (B, D)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5) multi-positive InfoNCE loss\n",
    "# =========================================\n",
    "def multipos_infonce_loss(e_anchor: torch.Tensor,\n",
    "                          e_pos1: torch.Tensor,\n",
    "                          e_pos2: torch.Tensor,\n",
    "                          temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    - anchors: (B, D)\n",
    "    - pos1, pos2: (B, D)\n",
    "    positives = {pos1[i], pos2[i]} for anchor[i]\n",
    "    negatives = ëª¨ë“  pos1/pos2 ì¤‘ ë‚˜ë¨¸ì§€ë“¤ (in-batch negative)\n",
    "    \"\"\"\n",
    "    B = e_anchor.size(0)\n",
    "    device = e_anchor.device\n",
    "\n",
    "    # (B, 2B) similarity\n",
    "    e_pos_all = torch.cat([e_pos1, e_pos2], dim=0)   # (2B, D)\n",
    "    logits = torch.matmul(e_anchor, e_pos_all.T) / temperature  # (B, 2B)\n",
    "\n",
    "    # ì•ˆì •ì ì¸ softmaxë¥¼ ìœ„í•´ max ë¹¼ê¸°\n",
    "    logits_max, _ = logits.max(dim=1, keepdim=True)\n",
    "    logits = logits - logits_max\n",
    "\n",
    "    exp_logits = torch.exp(logits)                  # (B, 2B)\n",
    "\n",
    "    # positive mask: (B, 2B)\n",
    "    pos_mask = torch.zeros_like(exp_logits)\n",
    "    idx = torch.arange(B, device=device)\n",
    "    pos_mask[idx, idx] = 1          # anchor i â†” pos1[i]\n",
    "    pos_mask[idx, idx + B] = 1      # anchor i â†” pos2[i]\n",
    "\n",
    "    pos_exp = (exp_logits * pos_mask).sum(dim=1)       # (B,)\n",
    "    all_exp = exp_logits.sum(dim=1)                    # (B,)\n",
    "\n",
    "    loss = -torch.log(pos_exp / all_exp + 1e-12).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6) MAIN\n",
    "# =========================================\n",
    "def main():\n",
    "\n",
    "    set_seed(SEED)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # GRCh38 ë¡œë“œ\n",
    "    # -----------------------------\n",
    "    if not os.path.exists(GRCH38_CSV):\n",
    "        raise FileNotFoundError(f\"GRCh38 CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {GRCH38_CSV}\")\n",
    "\n",
    "    grch_seqs = load_grch38_sequences_stream(GRCH38_CSV, max_samples=MAX_GRCH_SEQ)\n",
    "\n",
    "    # -----------------------------\n",
    "    # test ë¡œë“œ\n",
    "    # -----------------------------\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    test_seqs = test_df[\"seq\"].astype(str).tolist()\n",
    "    print(\"Test seqs:\", len(test_seqs))\n",
    "\n",
    "    # -----------------------------\n",
    "    # DNABERT-6 ë¡œë“œ\n",
    "    # -----------------------------\n",
    "    print(\"\\n[Load DNABERT-6 backbone]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        do_lower_case=False\n",
    "    )\n",
    "\n",
    "    backbone = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        output_hidden_states=True\n",
    "    ).to(device)\n",
    "\n",
    "    backbone.eval()\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False  # ë°±ë³¸ ì™„ì „ freeze\n",
    "\n",
    "    # -----------------------------\n",
    "    # Head ì •ì˜\n",
    "    # -----------------------------\n",
    "    head = DnaBertMultiLayerHead(\n",
    "        hidden_size=backbone.config.hidden_size,\n",
    "        last_n=LAST_N_LAYERS,\n",
    "        out_dim=OUTPUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        head.parameters(),\n",
    "        lr=LR_HEAD,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    scaler = amp.GradScaler(enabled=USE_FP16)\n",
    "\n",
    "    # -----------------------------\n",
    "    # TRAIN\n",
    "    # -----------------------------\n",
    "    print(\"\\n===== multi-positive InfoNCE TRAINING (GRCh38 only) =====\")\n",
    "\n",
    "    total_steps = (NUM_TUPLES // BATCH_SIZE_TR) * TRAIN_EPOCHS\n",
    "    step = 0\n",
    "\n",
    "    head.train()\n",
    "    for ep in range(TRAIN_EPOCHS):\n",
    "        print(f\"\\n[Epoch {ep+1}] Generating tuples...\")\n",
    "        tuples = generate_multi_pos_tuples(grch_seqs, NUM_TUPLES)\n",
    "        random.shuffle(tuples)\n",
    "\n",
    "        epoch_losses = []\n",
    "\n",
    "        for i in tqdm(range(0, NUM_TUPLES, BATCH_SIZE_TR), desc=f\"Epoch {ep+1}\"):\n",
    "            batch = tuples[i:i+BATCH_SIZE_TR]\n",
    "            if len(batch) < 2:\n",
    "                continue\n",
    "\n",
    "            anc, pos1, pos2 = zip(*batch)\n",
    "\n",
    "            # DNABERTìš© k-mer ë³€í™˜\n",
    "            anc_k  = [to_kmer(s, KMER) for s in anc]\n",
    "            pos1_k = [to_kmer(s, KMER) for s in pos1]\n",
    "            pos2_k = [to_kmer(s, KMER) for s in pos2]\n",
    "\n",
    "            # í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì³ì„œ í† í¬ë‚˜ì´ì¦ˆ\n",
    "            seqs_all = anc_k + pos1_k + pos2_k   # ê¸¸ì´ = 3B\n",
    "\n",
    "            batch_enc = tokenizer(\n",
    "                seqs_all,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with amp.autocast(device_type=\"cuda\", enabled=USE_FP16):\n",
    "                out = backbone(**batch_enc)\n",
    "                hidden_states = out.hidden_states   # list of (B', L, H)\n",
    "\n",
    "                emb_all = head(hidden_states, batch_enc[\"attention_mask\"])  # (3B, D)\n",
    "\n",
    "                B = len(batch)\n",
    "                e_anc  = emb_all[:B]\n",
    "                e_pos1 = emb_all[B:2*B]\n",
    "                e_pos2 = emb_all[2*B:3*B]\n",
    "\n",
    "                loss = multipos_infonce_loss(e_anc, e_pos1, e_pos2, TEMPERATURE)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "            step += 1\n",
    "\n",
    "        print(f\"[Epoch {ep+1}] InfoNCE loss = {np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # INFERENCE\n",
    "    # -----------------------------\n",
    "    print(\"\\n===== Inference on test.csv (original + RC, mean) =====\")\n",
    "    head.eval()\n",
    "\n",
    "    all_embs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(test_seqs), BATCH_SIZE_INFER), desc=\"Inference\"):\n",
    "            batch = test_seqs[i:i+BATCH_SIZE_INFER]\n",
    "\n",
    "            # ì›ë³¸/RC ì •ë¦¬\n",
    "            batch_clean = [clean_seq(s) for s in batch]\n",
    "            batch_rc    = [reverse_complement(s) for s in batch_clean]\n",
    "\n",
    "            batch_kmer    = [to_kmer(s, KMER) for s in batch_clean]\n",
    "            batch_rc_kmer = [to_kmer(s, KMER) for s in batch_rc]\n",
    "\n",
    "            seqs_all = batch_kmer + batch_rc_kmer\n",
    "\n",
    "            enc = tokenizer(\n",
    "                seqs_all,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            out = backbone(**enc)\n",
    "            hidden_states = out.hidden_states\n",
    "\n",
    "            emb_all = head(hidden_states, enc[\"attention_mask\"])  # (2B, D)\n",
    "            B = len(batch)\n",
    "            emb_v1 = emb_all[:B]\n",
    "            emb_v2 = emb_all[B:2*B]\n",
    "\n",
    "            emb_mean = (emb_v1 + emb_v2) / 2.0\n",
    "            all_embs.append(emb_mean.cpu().numpy())\n",
    "\n",
    "    all_embs = np.concatenate(all_embs, axis=0)  # (N_test, D)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save submission\n",
    "    # -----------------------------\n",
    "    col_names = [f\"emb_{i:04d}\" for i in range(OUTPUT_DIM)]\n",
    "    emb_df = pd.DataFrame(all_embs, columns=col_names)\n",
    "\n",
    "    final_df = pd.concat([test_df[[\"ID\"]], emb_df], axis=1)\n",
    "    final_df.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "    print(\"\\nâœ… Saved:\", OUT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ac243",
   "metadata": {},
   "source": [
    "### nucleotide-transformer-v2 ì½”ë“œ (ì ìˆ˜ : 0.539)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e814e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "SEED = 42\n",
    "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\yutt4\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¸ê³µì§€ëŠ¥\\open\"\n",
    "EXTERNAL_CSV = os.path.join(DATA_DIR, \"human_genome_train.csv\")\n",
    "GRCH_PATH    = os.path.join(DATA_DIR, \"grch38_windows_seq.csv\")\n",
    "TEST_PATH    = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SAMPLE_SUB_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "OUT_PATH = \"submission_heavy_finetune_contrastive_max128_ìˆ˜ì •ë³¸4.csv\"\n",
    "\n",
    "# í•µì‹¬ íŒŒë¼ë¯¸í„°\n",
    "MAX_LENGTH = 128\n",
    "OUTPUT_DIM = 512\n",
    "LAST_N_LAYERS = 12       # í™œìš© layer í™•ëŒ€\n",
    "\n",
    "TRAIN_EPOCHS = 10        # ë§¤ìš° ê¸¸ê²Œ\n",
    "NUM_TUPLES = 120000      # íŠœí”Œ í¬ê²Œ ì¦ê°€\n",
    "BATCH_SIZE_TR = 16\n",
    "BATCH_SIZE_INFER = 32\n",
    "\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "MAX_EXT_SEQ = 200_000\n",
    "MAX_GRCH_SEQ = 300_000\n",
    "\n",
    "USE_FP16 = True\n",
    "\n",
    "# ë°±ë³¸ íŒŒì¸íŠœë‹ ê¹Šê²Œ\n",
    "UNFREEZE_BACKBONE = True\n",
    "BACKBONE_LR = 1e-6\n",
    "BACKBONE_WEIGHT_DECAY = 1e-6\n",
    "\n",
    "# projection head í•™ìŠµë¥ \n",
    "LR_HEAD = 5e-4\n",
    "HEAD_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "\n",
    "# ======================\n",
    "# UTILS\n",
    "# ======================\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "\n",
    "\n",
    "def clean(seq: str) -> str:\n",
    "    seq = str(seq).upper().replace(\"N\", \"\")\n",
    "    return seq\n",
    "\n",
    "\n",
    "def reverse_complement(seq: str) -> str:\n",
    "    return seq.translate(str.maketrans(\"ACGT\", \"TGCA\"))[::-1]\n",
    "\n",
    "\n",
    "def apply_snv(seq: str, k: int):\n",
    "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    arr = list(seq)\n",
    "    idxs = random.sample(range(len(arr)), min(len(arr), k))\n",
    "    for i in idxs:\n",
    "        orig = arr[i]\n",
    "        cand = [b for b in bases if b != orig]\n",
    "        arr[i] = random.choice(cand)\n",
    "    return \"\".join(arr)\n",
    "\n",
    "\n",
    "def crop(seq: str):\n",
    "    if len(seq) <= MAX_LENGTH:\n",
    "        return seq\n",
    "    s = random.randint(0, len(seq)-MAX_LENGTH)\n",
    "    return seq[s:s+MAX_LENGTH]\n",
    "\n",
    "\n",
    "def load_grch_stream(path, max_samples, chunksize=200000):\n",
    "    seqs = []\n",
    "    print(\"\\n[GRCh38 STREAM LOAD]\", path)\n",
    "    for chunk in pd.read_csv(path, usecols=[\"seq\"], chunksize=chunksize):\n",
    "        tmp = [clean(s) for s in chunk[\"seq\"].tolist()]\n",
    "        tmp = [s for s in tmp if len(s) > 0]\n",
    "\n",
    "        need = max_samples - len(seqs)\n",
    "        if need <= 0:\n",
    "            break\n",
    "        if len(tmp) > need:\n",
    "            tmp = tmp[:need]\n",
    "\n",
    "        seqs.extend(tmp)\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "        if len(seqs) >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(\"Loaded GRCh38:\", len(seqs))\n",
    "    return seqs\n",
    "\n",
    "\n",
    "# ======================\n",
    "# TUPLE GENERATION (ê°•í•œ ë²„ì „)\n",
    "# ======================\n",
    "def generate_contrastive_tuples(seqs: List[str], N: int):\n",
    "    \"\"\"\n",
    "    anchor: ì›ë³¸ crop\n",
    "    pos   : SNV 1ê°œ\n",
    "    neg   : SNV 8~12ê°œ\n",
    "    â†’ SNV ë¯¼ê°ë„ ê·¹ëŒ€í™” ì „ëµ\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    print(f\"Generating {N} contrastive tuples (strong SNV)\")\n",
    "    while len(rows) < N:\n",
    "        a = crop(clean(random.choice(seqs)))\n",
    "        if len(a) == 0:\n",
    "            continue\n",
    "        pos = crop(apply_snv(a, 1))\n",
    "        neg = crop(apply_snv(a, random.randint(8, 12)))\n",
    "        rows.append((a, pos, neg))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ======================\n",
    "# MODEL HEAD\n",
    "# ======================\n",
    "def l2_norm(x):\n",
    "    return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-12)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, hid, last_n, out):\n",
    "        super().__init__()\n",
    "        self.last_n = last_n\n",
    "        self.w = nn.Parameter(torch.zeros(last_n))\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hid, hid*2),\n",
    "            nn.LayerNorm(hid*2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hid*2, out),\n",
    "        )\n",
    "\n",
    "    def forward(self, hs, mask):\n",
    "        stack = torch.stack(hs[-self.last_n:], dim=0)   # (L, B, T, H)\n",
    "        w = F.softmax(self.w, dim=0).view(-1,1,1,1)\n",
    "        feat = (stack * w).sum(dim=0)                   # (B, T, H)\n",
    "\n",
    "        mask = mask.unsqueeze(-1).float()\n",
    "        pooled = (feat * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        return l2_norm(self.proj(pooled))\n",
    "\n",
    "\n",
    "def contrastive_loss(ea, ep, en, temp=0.1):\n",
    "    B = ea.size(0)\n",
    "    all_e = torch.cat([ea, ep, en], dim=0)\n",
    "    sim = torch.matmul(ea, all_e.T) / temp\n",
    "    labels = torch.arange(B, 2*B, device=ea.device)\n",
    "    return F.cross_entropy(sim, labels)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# MAIN\n",
    "# ======================\n",
    "def main():\n",
    "\n",
    "    set_seed(SEED)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # ------------------------\n",
    "    # Load external data\n",
    "    # ------------------------\n",
    "    df = pd.read_csv(EXTERNAL_CSV)\n",
    "    ext = [clean(s) for s in df[\"seq\"].tolist()]\n",
    "    ext = [s for s in ext if len(s) > 0]\n",
    "    if len(ext) > MAX_EXT_SEQ:\n",
    "        idx = np.random.choice(len(ext), MAX_EXT_SEQ, replace=False)\n",
    "        ext = [ext[i] for i in idx]\n",
    "\n",
    "    # ------------------------\n",
    "    # Load GRCh38 stream\n",
    "    # ------------------------\n",
    "    grch = load_grch_stream(GRCH_PATH, MAX_GRCH_SEQ)\n",
    "\n",
    "    # training sequences = ext + grch\n",
    "    all_seqs = ext + grch\n",
    "    print(\"Total training seqs:\", len(all_seqs))\n",
    "\n",
    "    # ------------------------\n",
    "    # Load test\n",
    "    # ------------------------\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    test_seqs = [clean(s) for s in test_df[\"seq\"].tolist()]\n",
    "    print(\"Test seqs:\", len(test_seqs))\n",
    "\n",
    "    # ------------------------\n",
    "    # Load backbone\n",
    "    # ------------------------\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    gLM = AutoModelForMaskedLM.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    gLM.to(device)\n",
    "\n",
    "    # Freeze all first\n",
    "    for p in gLM.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # ğŸ”¥ Unfreeze LAST 8 LAYERS: 24~31\n",
    "    if UNFREEZE_BACKBONE:\n",
    "        for name, p in gLM.named_parameters():\n",
    "            if any(f\"encoder.layer.{i}\" in name for i in range(24, 32)):\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # ------------------------\n",
    "    # HEAD\n",
    "    # ------------------------\n",
    "    model = Head(\n",
    "        hid=gLM.config.hidden_size,\n",
    "        last_n=LAST_N_LAYERS,\n",
    "        out=OUTPUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    # ------------------------\n",
    "    # OPTIMIZER + SCHEDULER\n",
    "    # ------------------------\n",
    "    params = [\n",
    "        {\"params\": model.parameters(), \"lr\": LR_HEAD, \"weight_decay\": HEAD_WEIGHT_DECAY},\n",
    "        {\"params\": [p for p in gLM.parameters() if p.requires_grad],\n",
    "         \"lr\": BACKBONE_LR, \"weight_decay\": BACKBONE_WEIGHT_DECAY}\n",
    "    ]\n",
    "\n",
    "    optim = torch.optim.AdamW(params)\n",
    "    total_steps = (NUM_TUPLES // BATCH_SIZE_TR) * TRAIN_EPOCHS\n",
    "    warmup_steps = total_steps // 10  # 10% warmup\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optim,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    scaler = amp.GradScaler(enabled=USE_FP16)\n",
    "\n",
    "    # ==========================\n",
    "    # TRAIN LOOP\n",
    "    # ==========================\n",
    "    for ep in range(TRAIN_EPOCHS):\n",
    "        print(f\"\\n[Epoch {ep+1}] Generating tuples...\")\n",
    "        tuples = generate_contrastive_tuples(all_seqs, NUM_TUPLES)\n",
    "        random.shuffle(tuples)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in tqdm(range(0, NUM_TUPLES, BATCH_SIZE_TR)):\n",
    "            batch = tuples[i:i+BATCH_SIZE_TR]\n",
    "            if len(batch) < 2: continue\n",
    "\n",
    "            anc, pos, neg = zip(*batch)\n",
    "            seqs = list(anc) + list(pos) + list(neg)\n",
    "\n",
    "            enc = tok(\n",
    "                seqs,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            with amp.autocast(device_type=\"cuda\", enabled=USE_FP16):\n",
    "                out = gLM(**enc, output_hidden_states=True)\n",
    "                emb = model(out.hidden_states, enc[\"attention_mask\"])\n",
    "\n",
    "                B = len(batch)\n",
    "                ea = emb[:B]\n",
    "                ep = emb[B:2*B]\n",
    "                en = emb[2*B:3*B]\n",
    "\n",
    "                loss = contrastive_loss(ea, ep, en, TEMPERATURE)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f\"[Epoch {ep+1}] loss:\", np.mean(losses))\n",
    "\n",
    "    # ==========================\n",
    "    # INFERENCE\n",
    "    # ==========================\n",
    "    print(\"\\n[Inference]\")\n",
    "\n",
    "    final_embs = []\n",
    "    for i in tqdm(range(0, len(test_seqs), BATCH_SIZE_INFER)):\n",
    "        batch = test_seqs[i:i+BATCH_SIZE_INFER]\n",
    "\n",
    "        enc = tok(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = gLM(**enc, output_hidden_states=True)\n",
    "            emb = model(out.hidden_states, enc[\"attention_mask\"])\n",
    "            final_embs.append(emb.cpu().numpy())\n",
    "\n",
    "    final_embs = np.concatenate(final_embs, axis=0)\n",
    "    cols = [f\"emb_{i:04d}\" for i in range(OUTPUT_DIM)]\n",
    "    df_emb = pd.DataFrame(final_embs, columns=cols)\n",
    "\n",
    "    out = pd.concat([test_df[[\"ID\"]], df_emb], axis=1)\n",
    "    out.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\", OUT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
