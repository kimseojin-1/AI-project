{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3c1692",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfd2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======================\n",
    "# ì„¤ì • ë¶€ë¶„\n",
    "# ======================\n",
    "SEED = 42\n",
    "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "\n",
    "# ğŸ‘‰ ëŒ€íšŒ ì œê³µ ë°ì´í„° ê²½ë¡œ (test.csv, sample_submission.csv ìœ„ì¹˜)\n",
    "DATA_DIR = r\"C:\\Users\\yutt4\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¸ê³µì§€ëŠ¥\\open\"\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SAMPLE_SUB_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# ğŸ‘‰ ì™¸ë¶€ ìœ ì „ì²´ ìœˆë„ìš° ë°ì´í„° ê²½ë¡œ \n",
    "# ì˜ˆ: id, chrom, start_pos, seq ì»¬ëŸ¼ ìˆëŠ” íŒŒì¼\n",
    "EXTERNAL_CSV = os.path.join(DATA_DIR, \"grch38_windows_seq.csv\")  \n",
    "\n",
    "OUT_PATH = \"submission_infonce_snvs_multiview_v2.csv\"\n",
    "\n",
    "OUTPUT_DIM = 2048          # ì œì¶œ ì„ë² ë”© ì°¨ì›\n",
    "LAST_N_LAYERS = 12         # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ ì‚¬ìš© (12 ì •ë„ê°€ ì‹¤ìš©ì )\n",
    "MAX_LENGTH = 512\n",
    "USE_FP16 = True\n",
    "\n",
    "BATCH_SIZE_TR = 16         # í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ (GPU ë³´ê³  ì¡°ì ˆ)\n",
    "BATCH_SIZE_INFER = 32      # ì¶”ë¡  ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "\n",
    "# ğŸ”§ í•™ìŠµ ê°•ë„ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "TRAIN_EPOCHS = 2           # GPU í˜ë“¤ë©´ 1ë¡œ ì¤„ì—¬ë„ ë¨\n",
    "LR_HEAD = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "TEMPERATURE = 0.1          # InfoNCE ì˜¨ë„ (0.05~0.2 ì‚¬ì´ íŠœë‹ ê°€ëŠ¥)\n",
    "NUM_PAIRS = 20000          # InfoNCE í•™ìŠµìš© anchorâ€“positive pair ê°œìˆ˜\n",
    "\n",
    "# ì™¸ë¶€ ë°ì´í„°ì—ì„œ ìµœëŒ€ ëª‡ ê°œ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í• ì§€ (ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§)\n",
    "MAX_EXT_SEQ = 200_000      # GPU/ì‹œê°„ ë³´ê³  ì¡°ì ˆ (ì˜ˆ: 50k, 100k, 200k ë“±)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# ìœ í‹¸ í•¨ìˆ˜ë“¤\n",
    "# ======================\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    return x / x.norm(p=2, dim=-1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "# reverse complementìš© ë§¤í•‘\n",
    "_rc_map = str.maketrans(\"ACGT\", \"TGCA\")\n",
    "\n",
    "\n",
    "def reverse_complement(seq: str) -> str:\n",
    "    \"\"\"ì—¼ê¸°ì„œì—´ì˜ reverse complement.\"\"\"\n",
    "    return seq.translate(_rc_map)[::-1]\n",
    "\n",
    "\n",
    "def apply_biological_snv(seq: str, num_mutations: int) -> str:\n",
    "    \"\"\"\n",
    "    ì‹œí€€ìŠ¤ì—ì„œ ì„ì˜ ìœ„ì¹˜ num_mutations ê°œë¥¼\n",
    "    ë‹¤ë¥¸ ì—¼ê¸°ë¡œ ë°”ê¾¸ëŠ” ê°„ë‹¨í•œ SNV(ë³€ì´) ìƒì„± í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    s = list(seq)\n",
    "    if len(s) == 0:\n",
    "        return seq\n",
    "\n",
    "    idx_list = random.sample(range(len(s)), min(num_mutations, len(s)))\n",
    "    for idx in idx_list:\n",
    "        orig = s[idx]\n",
    "        candidates = [b for b in bases if b != orig]\n",
    "        if not candidates:\n",
    "            continue\n",
    "        s[idx] = random.choice(candidates)\n",
    "    return \"\".join(s)\n",
    "\n",
    "\n",
    "def generate_pairs_with_augmentation(sequences: List[str], num_pairs: int):\n",
    "    \"\"\"\n",
    "    InfoNCE ìš© anchorâ€“positive ìŒ ìƒì„± (ê°œì„  ë²„ì „).\n",
    "    - anchor: ì™¸ë¶€ seq ì¤‘ í•˜ë‚˜ (ê¸¸ë©´ 512ë¡œ ì˜ë¼ì„œ ì‚¬ìš©)\n",
    "    - positive:\n",
    "        * 40%: anchor ê·¸ëŒ€ë¡œ\n",
    "        * 40%: anchor + 1~2ê°œì˜ SNV (ì•„ì£¼ ì‘ì€ ë³€ì´)\n",
    "        * 20%: reverse complement(anchor ë˜ëŠ” SNV ì ìš© í›„)\n",
    "    -> ì‘ì€ ë³€ì´ì—ë„ 'ê°€ê¹Œìš´' ì„ë² ë”©ì„ ë§Œë“¤ë„ë¡ ìœ ë„.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    print(f\"Generating {num_pairs} (anchor, positive) pairs for InfoNCE\")\n",
    "    for _ in range(num_pairs):\n",
    "        anchor = random.choice(sequences)\n",
    "\n",
    "        # anchor ê¸¸ì´ ì œí•œ\n",
    "        if len(anchor) > MAX_LENGTH:\n",
    "            start = random.randint(0, len(anchor) - MAX_LENGTH)\n",
    "            anchor = anchor[start:start + MAX_LENGTH]\n",
    "\n",
    "        r = random.random()\n",
    "        if r < 0.4:\n",
    "            # 40%: ì™„ì „íˆ ë™ì¼\n",
    "            pos = anchor\n",
    "        elif r < 0.8:\n",
    "            # 40%: ì•„ì£¼ ì‘ì€ SNV (1~2ê°œ)\n",
    "            mutated = apply_biological_snv(anchor, num_mutations=random.randint(1, 2))\n",
    "            pos = mutated\n",
    "        else:\n",
    "            # 20%: reverse complement\n",
    "            if random.random() < 0.5:\n",
    "                pos = reverse_complement(anchor)\n",
    "            else:\n",
    "                mutated = apply_biological_snv(anchor, num_mutations=random.randint(1, 2))\n",
    "                pos = reverse_complement(mutated)\n",
    "\n",
    "        rows.append([anchor, pos])\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ======================\n",
    "# ì„ë² ë”© í—¤ë“œ ì •ì˜\n",
    "# ======================\n",
    "class RobustModel(nn.Module):\n",
    "    \"\"\"\n",
    "    - gLMì˜ hidden_states ì¤‘ ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ì— learnable weight ë¶€ì—¬\n",
    "    - weighted sum í›„, attention mask ê¸°ë°˜ mean pooling\n",
    "    - projection MLP í†µí•´ OUTPUT_DIM ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜\n",
    "    - ë§ˆì§€ë§‰ L2 normalize\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, last_n: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.last_n = last_n\n",
    "        self.layer_weights = nn.Parameter(torch.zeros(last_n))  # learnable layer weights\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size * 2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        hidden_states: list of (B, L, H), ê¸¸ì´ = num_layers+1 (embedding í¬í•¨)\n",
    "        mask: (B, L)\n",
    "        \"\"\"\n",
    "        # ë§ˆì§€ë§‰ last_n ë ˆì´ì–´ë§Œ ì‚¬ìš©\n",
    "        stack = torch.stack(hidden_states[-self.last_n:], dim=0)  # (last_n, B, L, H)\n",
    "\n",
    "        # softmaxë¡œ ë ˆì´ì–´ ê°€ì¤‘ì¹˜\n",
    "        w = F.softmax(self.layer_weights, dim=0).view(-1, 1, 1, 1)\n",
    "        feat = (stack * w).sum(dim=0)  # (B, L, H)\n",
    "\n",
    "        # attention mask ê¸°ë°˜ mean pooling\n",
    "        mask_expanded = mask.unsqueeze(-1).float()  # (B, L, 1)\n",
    "        sum_embeddings = torch.sum(feat * mask_expanded, dim=1)  # (B, H)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # (B, 1)\n",
    "        mean_emb = sum_embeddings / sum_mask  # (B, H)\n",
    "\n",
    "        # projection + L2 normalize\n",
    "        return l2_normalize(self.proj(mean_emb))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# ë©”ì¸\n",
    "# ======================\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # --------------------------------\n",
    "    # 1) ì™¸ë¶€ ë°ì´í„° ë¡œë“œ (í•™ìŠµìš©)\n",
    "    # --------------------------------\n",
    "    print(f\"\\n[ì™¸ë¶€ ë°ì´í„° ë¡œë“œ] {EXTERNAL_CSV}\")\n",
    "    ext_df = pd.read_csv(EXTERNAL_CSV)\n",
    "    if \"seq\" not in ext_df.columns:\n",
    "        raise ValueError(\"ì™¸ë¶€ CSVì— 'seq' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    ext_sequences = ext_df[\"seq\"].astype(str).tolist()\n",
    "    print(\"External sequences (ì›ë³¸ ê°œìˆ˜):\", len(ext_sequences))\n",
    "\n",
    "    # ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§\n",
    "    if len(ext_sequences) > MAX_EXT_SEQ:\n",
    "        idx = np.random.choice(len(ext_sequences), size=MAX_EXT_SEQ, replace=False)\n",
    "        ext_sequences = [ext_sequences[i] for i in idx]\n",
    "        print(\"External sequences (ìƒ˜í”Œë§ í›„):\", len(ext_sequences))\n",
    "\n",
    "    # InfoNCEìš© anchorâ€“positive pair ìƒì„± (ì™¸ë¶€ ë°ì´í„°ë¡œë§Œ)\n",
    "    pair_data = generate_pairs_with_augmentation(\n",
    "        ext_sequences,\n",
    "        num_pairs=NUM_PAIRS\n",
    "    )\n",
    "\n",
    "    # --------------------------------\n",
    "    # 2) test.csv ë¡œë“œ (ì¶”ë¡ ìš©, í•™ìŠµì—ëŠ” ì ˆëŒ€ ì‚¬ìš© X)\n",
    "    # --------------------------------\n",
    "    print(f\"\\n[test ë°ì´í„° ë¡œë“œ] {TEST_PATH}\")\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    test_sequences = test_df[\"seq\"].astype(str).tolist()\n",
    "    print(\"Test sequences:\", len(test_sequences))\n",
    "\n",
    "    # --------------------------------\n",
    "    # 3) ì‚¬ì „í•™ìŠµ gLM ë°±ë³¸ ë¡œë“œ (freeze)\n",
    "    # --------------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    backbone = AutoModelForMaskedLM.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    backbone.to(device)\n",
    "    backbone.eval()\n",
    "\n",
    "    # ë°±ë³¸ íŒŒë¼ë¯¸í„°ëŠ” freeze\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # --------------------------------\n",
    "    # 4) ì„ë² ë”© í—¤ë“œ ì •ì˜\n",
    "    # --------------------------------\n",
    "    model = RobustModel(\n",
    "        hidden_size=backbone.config.hidden_size,\n",
    "        last_n=LAST_N_LAYERS,\n",
    "        out_dim=OUTPUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "    use_fp16_amp = (USE_FP16 and device == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16_amp)\n",
    "\n",
    "    # --------------------------------\n",
    "    # 5) InfoNCE í•™ìŠµ ë£¨í”„ (ì™¸ë¶€ ë°ì´í„° ê¸°ë°˜)\n",
    "    # --------------------------------\n",
    "    print(\"\\n===== InfoNCE í•™ìŠµ ì‹œì‘ (external data only) =====\")\n",
    "    model.train()\n",
    "\n",
    "    bs = BATCH_SIZE_TR\n",
    "    for epoch in range(TRAIN_EPOCHS):\n",
    "        random.shuffle(pair_data)\n",
    "        epoch_loss = []\n",
    "\n",
    "        for i in tqdm(range(0, len(pair_data), bs), desc=f\"Epoch {epoch+1}\"):\n",
    "            batch = pair_data[i:i + bs]\n",
    "            if len(batch) < 2:  # ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ\n",
    "                continue\n",
    "\n",
    "            anchors, poss = zip(*batch)   # tuple of strings\n",
    "            all_seqs = list(anchors) + list(poss)  # ê¸¸ì´ = 2B\n",
    "\n",
    "            # í† í¬ë‚˜ì´ì¦ˆ\n",
    "            enc = tokenizer(\n",
    "                all_seqs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_fp16_amp):\n",
    "                # ë°±ë³¸ì—ì„œ hidden states ì¶”ì¶œ (freeze ìƒíƒœ)\n",
    "                with torch.no_grad():\n",
    "                    out = backbone(**enc, output_hidden_states=True)\n",
    "\n",
    "                embs = model(out.hidden_states, enc[\"attention_mask\"])  # (2B, D)\n",
    "                B = len(batch)\n",
    "                ea, ep = embs[:B], embs[B:2 * B]  # anchor, positive\n",
    "\n",
    "                # InfoNCE / NT-Xent ìŠ¤íƒ€ì¼ loss\n",
    "                sim = torch.matmul(ea, ep.T)  # (B, B) - ì´ë¯¸ L2 normalizedë¼ cosine similarity\n",
    "\n",
    "                # temperature scaling\n",
    "                sim = sim / TEMPERATURE\n",
    "\n",
    "                # ê° anchor iì˜ íƒ€ê²Ÿì€ positive i (ëŒ€ê°ì„ )\n",
    "                labels = torch.arange(B, device=device)\n",
    "\n",
    "                # anchorâ†’positive ë°©í–¥\n",
    "                loss1 = F.cross_entropy(sim, labels)\n",
    "                # positiveâ†’anchor ë°©í–¥ (ëŒ€ì¹­ í•™ìŠµ)\n",
    "                loss2 = F.cross_entropy(sim.T, labels)\n",
    "                loss = (loss1 + loss2) / 2.0\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} InfoNCE Loss: {np.mean(epoch_loss):.4f}\")\n",
    "\n",
    "    # --------------------------------\n",
    "    # 6) ì¶”ë¡ : multi-view (original + reverse complement) on test.csv\n",
    "    # --------------------------------\n",
    "    print(\"\\n===== ì¶”ë¡  ì‹œì‘ (multi-view: original + RC, test.csv) =====\")\n",
    "    model.eval()\n",
    "    sub_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(test_sequences), BATCH_SIZE_INFER), desc=\"Inference\"):\n",
    "        batch = test_sequences[i:i + BATCH_SIZE_INFER]\n",
    "\n",
    "        # view 1: ì›ë³¸\n",
    "        batch_v1 = batch\n",
    "        # view 2: reverse complement\n",
    "        batch_v2 = [reverse_complement(s) for s in batch]\n",
    "\n",
    "        all_views = batch_v1 + batch_v2   # ê¸¸ì´ = 2B\n",
    "        enc = tokenizer(\n",
    "            all_views,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = backbone(**enc, output_hidden_states=True)\n",
    "            emb_all = model(out.hidden_states, enc[\"attention_mask\"])  # (2B, D)\n",
    "\n",
    "            B = len(batch)\n",
    "            emb_v1 = emb_all[:B]\n",
    "            emb_v2 = emb_all[B:2 * B]\n",
    "\n",
    "            # ë‘ view í‰ê· \n",
    "            emb_mean = (emb_v1 + emb_v2) / 2.0\n",
    "            embeddings.append(emb_mean.cpu().numpy())\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)  # (N, OUTPUT_DIM)\n",
    "\n",
    "    # --------------------------------\n",
    "    # 7) ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    # --------------------------------\n",
    "    col_names = [f\"emb_{i:04d}\" for i in range(OUTPUT_DIM)]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=col_names)\n",
    "    final_df = pd.concat([sub_df[[\"ID\"]], emb_df], axis=1)\n",
    "    final_df.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {OUT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
